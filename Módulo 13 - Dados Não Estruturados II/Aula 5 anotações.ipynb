{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9afeba56-8896-47c9-9e51-e6e1203ba807",
   "metadata": {},
   "source": [
    "## Aula 5 - Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40ec8461-1483-44c8-a4bd-fb1530f79618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding\n",
    "from gensim import *\n",
    "import spacy\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66fbb022-aa24-4c89-836a-9a64661427c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext-wiki-news-subwords-300\n",
      "conceptnet-numberbatch-17-06-300\n",
      "word2vec-ruscorpora-300\n",
      "word2vec-google-news-300\n",
      "glove-wiki-gigaword-50\n",
      "glove-wiki-gigaword-100\n",
      "glove-wiki-gigaword-200\n",
      "glove-wiki-gigaword-300\n",
      "glove-twitter-25\n",
      "glove-twitter-50\n",
      "glove-twitter-100\n",
      "glove-twitter-200\n",
      "__testing_word2vec-matrix-synopsis\n"
     ]
    }
   ],
   "source": [
    "for model_name in list(gensim.downloader.info()[\"models\"].keys()):\n",
    "    print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47366f5e-b22a-4657-8916-319268aee793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 99.0% 1646.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "g_news = gensim.downloader.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2996d681-23c0-4d64-9d26-9c0072db5e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paris está para França, assim como Londres está para... (Inglaterra)\n",
    "# (Paris - França) + Londres = Inglaterra (positive=[\"Paris\", \"Londres\"], negative=[\"França\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd591ad-b6ba-441b-a94c-da309eac9427",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658e4449-a558-4de0-a666-2d73383363c5",
   "metadata": {},
   "source": [
    "### Usando spacy\n",
    "\n",
    "https://stackoverflow.com/questions/63693463/spacy-lemmatizer-removes-capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d213229-3736-4b18-9026-3dc97ea64aa5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.28.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.59.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.20.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2020.12.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-31 19:53:15.651849: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-08-31 19:53:15.652776: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-08-31 19:53:24.976641: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-08-31 19:53:24.977456: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found\n",
      "2022-08-31 19:53:24.978228: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found\n",
      "2022-08-31 19:53:24.978984: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\n",
      "2022-08-31 19:53:24.979744: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\n",
      "2022-08-31 19:53:24.980503: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusolver64_11.dll'; dlerror: cusolver64_11.dll not found\n",
      "2022-08-31 19:53:24.981241: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found\n",
      "2022-08-31 19:53:24.981959: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\n",
      "2022-08-31 19:53:24.982007: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "WARNING: Error parsing requirements for retrying: [Errno 2] No such file or directory: 'c:\\\\users\\\\gabri\\\\anaconda3\\\\lib\\\\site-packages\\\\retrying-1.3.3.dist-info\\\\METADATA'\n"
     ]
    }
   ],
   "source": [
    "# baixando um modelo do spacy já treinada de palavras em inglês\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "686392ac-e444-43da-bdd8-ea22e414f05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar um pipeline de modelo treinado 'en_core_web_sm' \n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ce3c611-5b12-4a51-bfdb-c75fb0a3bcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo nlp com a string\n",
    "text = nlp(\"I don't know. Professor of Data Science.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d89a389b-3ea2-4ac4-abe0-27b58f4653b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['know', 'professor', 'data', 'science']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenização com lemma, lowercase, sem stopwords e sem pontuação\n",
    "txt = [token.lemma_.lower() for token in text if not token.is_stop and not token.is_punct]\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65976d4-2147-4e3c-b0c5-8eebde0441ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como função\n",
    "def clear(text):\n",
    "    global txt = [token.lemma_.lower() for token in text if not token.is_stop and not token.is_punct]\n",
    "    if len(txt) > 2:\n",
    "        return \" \".join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b51baeb-d787-413e-abb6-5a378a33ec7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRP', 'VBP', 'RB', 'VB', '.', 'NNP', 'IN', 'NNP', 'NNP', '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tag names : https://machinelearningknowledge.ai/tutorial-on-spacy-part-of-speech-pos-tagging/\n",
    "txt2 = [token.tag_ for token in text]\n",
    "txt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdfaac7-eafe-4ee8-915c-ec26f80d6262",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62acc265-d4a1-45b8-a138-41f367f8df4f",
   "metadata": {},
   "source": [
    "### G_news: modelo já treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "101a3574-ace2-4431-a32a-845a24bd387f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Milan', 0.7222141623497009),\n",
       " ('Rome', 0.702830970287323),\n",
       " ('Palermo_Sicily', 0.5967569947242737),\n",
       " ('Italian', 0.5911272764205933),\n",
       " ('Tuscany', 0.5632812976837158),\n",
       " ('Bologna', 0.5608358383178711),\n",
       " ('Sicily', 0.5596384406089783),\n",
       " ('Bologna_Italy', 0.5470058917999268),\n",
       " ('Berna_Milan', 0.5464026927947998),\n",
       " ('Genoa', 0.5308900475502014)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capital = g_news.most_similar([\"Paris\", \"Italy\"], [\"France\"])\n",
    "capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cb1bb59-7ffc-4c4a-b32d-b087dd420768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Audi', 0.760235071182251),\n",
       " ('BMW', 0.7272197604179382),\n",
       " ('Maserati', 0.7194910645484924),\n",
       " ('VW', 0.6961471438407898)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = g_news.most_similar('Porsche', topn = 4)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16ee3770-6191-447e-a6c7-78ec5c97bb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5555642"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = g_news.similarity('Italy', 'Rome')\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3fbdee-6ca8-47cf-8f04-e8b566e431e8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae336a8-899a-4345-9843-dd9dfccfcf59",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cd06e69-b274-4c42-b2a8-e8a459f878f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "959f76b6-dc88-48f8-94da-11ccd3726c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/movies.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d3e355b-be7b-482a-b14f-1916837f2438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I grew up (b. 1965) watching and loving the Th...      0\n",
       "1  When I put this movie in my DVD player, and sa...      0\n",
       "2  Why do people who do not know what a particula...      0\n",
       "3  Even though I have great interest in Biblical ...      0\n",
       "4  Im a die hard Dads Army fan and nothing will e...      1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c4ec7a9-d080-46ac-b61c-5c2554eb9b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_sample = df.sample(frac = 0.2, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f56246b1-7537-4f66-94f3-670f617bfba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_sample = movies_sample.reset_index(drop = True)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0aea487-57fd-4fc1-a1a5-0a0d4c1ce7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(string):\n",
    "    \"\"\"\n",
    "    Preprocessing for english'\n",
    "    \"\"\"\n",
    "    # Manter somente caracteres e números - sem caracteres especiais\n",
    "    string = re.sub(r\"[^a-zA-Z0-9]+\", \" \", string)\n",
    "    # Letras minúsculas\n",
    "    string = string.lower()\n",
    "    #\n",
    "    txt = [token.lemma_.lower() for token in string if not token.is_stop and not token.is_punct]\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d624205-2db9-41d5-8616-68ca5a25076a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# melhor usar o antigo com stem e lemma\n",
    "movies_sample['filtered_words'] = movies_sample[\"text\"].apply(lambda x: nlp(x))\n",
    "movies_sample['filtered_words'] = movies_sample[\"filtered_words\"].apply(lambda x: preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2a4b74c5-dd24-4959-ae17-ffa0827193e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>filtered_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love this movie. The cast were all terrific ...</td>\n",
       "      <td>1</td>\n",
       "      <td>(I, love, this, movie, ., The, cast, were, all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Lone Ranger &amp; Tonto set out to bring to ju...</td>\n",
       "      <td>1</td>\n",
       "      <td>(The, Lone, Ranger, &amp;, Tonto, set, out, to, br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;According to reviewers, the year i...</td>\n",
       "      <td>0</td>\n",
       "      <td>(&lt;, br, /&gt;&lt;br, /&gt;According, to, reviewers, ,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Has anyone noticed that James Earl Jones is th...</td>\n",
       "      <td>1</td>\n",
       "      <td>(Has, anyone, noticed, that, James, Earl, Jone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is just a case of a previously worthless ...</td>\n",
       "      <td>0</td>\n",
       "      <td>(This, is, just, a, case, of, a, previously, w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  I love this movie. The cast were all terrific ...      1   \n",
       "1  The Lone Ranger & Tonto set out to bring to ju...      1   \n",
       "2  <br /><br />According to reviewers, the year i...      0   \n",
       "3  Has anyone noticed that James Earl Jones is th...      1   \n",
       "4  This is just a case of a previously worthless ...      0   \n",
       "\n",
       "                                      filtered_words  \n",
       "0  (I, love, this, movie, ., The, cast, were, all...  \n",
       "1  (The, Lone, Ranger, &, Tonto, set, out, to, br...  \n",
       "2  (<, br, /><br, />According, to, reviewers, ,, ...  \n",
       "3  (Has, anyone, noticed, that, James, Earl, Jone...  \n",
       "4  (This, is, just, a, case, of, a, previously, w...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "843641bb-e190-4484-ba5b-46ef48b40ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = movies_sample[\"filtered_words\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e2cfb2b-aa8a-42cd-bb99-5f9104dabcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phraser, Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fbdb418b-9bc3-493e-86e1-b7a1c3f1affd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-acaf0160c0b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    425\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             self.train(\n\u001b[0m\u001b[0;32m    428\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_training_sanity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[1;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[0;32m   1538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1539\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# should be set by `build_vocab`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1540\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1541\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1542\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(words_list, min_count = 3, workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "04bbaf76-3352-4fc0-aa44-e7a8ea58cc94",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-fb4d1a5def39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c476f5aa-a473-4759-b916-ba26c9abfb1f",
   "metadata": {},
   "source": [
    "### Salvando model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9721ad7e-70a2-4a2e-a5b2-44755bd26acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_w2v.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e8b271-caff-4d99-9647-3014b9b86eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Word2Vec.load('model_w2v.bin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
