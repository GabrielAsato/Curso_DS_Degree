{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d484d2f-94d1-4637-ba2f-912fbf9d3ae8",
   "metadata": {},
   "source": [
    "### Projeto Módulo 13 - Análise de sentimento no Twitter\n",
    "\n",
    "https://www.kaggle.com/code/akshayarajasekaran/cd-sentiment-analysis-of-tweets/notebook\n",
    "\n",
    "O dataset contém 1_600_000 tweets extraido pelo Twitter API. É classificado por gradação (0: Negativo e 4: Positivo) uqe podem ser usados para detectar sentimentos.\n",
    "Contém 6 campos:\n",
    "- sentiment : polarização do tweet (0= negativo a 4=positivo)\n",
    "- ids\n",
    "- date : data no formato (Sat May 16 23:58:44 UTC 2009)\n",
    "- flag : The query\n",
    "- user : usuário\n",
    "- text : texto do tweet\n",
    "\n",
    "É necessário apenas a análise do text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc84922d-da25-41a5-b187-d26e48f075f6",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7e5d50-e4ed-4ec7-a2f0-c43321d0520f",
   "metadata": {},
   "source": [
    "### Pré-análise e configuração do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "133166f7-2a11-40b7-a183-2d80db253ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340405f3-7bf7-431a-a528-602b4670eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../twitter_proj/training.1600000.processed.noemoticon.csv\"\n",
    "dataset = pd.read_csv(path, names = [\"label\", \"ids\", \"date\", \"flag\", \"user\", \"tweet\"], encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9cdaaf-8bac-4284-9f0c-86f0162c4dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0</td>\n",
       "      <td>1467844157</td>\n",
       "      <td>Mon Apr 06 22:28:32 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AKyarnie</td>\n",
       "      <td>@onemoreproject that is lame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0</td>\n",
       "      <td>1467844505</td>\n",
       "      <td>Mon Apr 06 22:28:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>luimoral85</td>\n",
       "      <td>I don't understand... I really don't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0</td>\n",
       "      <td>1467844540</td>\n",
       "      <td>Mon Apr 06 22:28:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ceironous</td>\n",
       "      <td>HEROES just isn't doing it for me this season...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0</td>\n",
       "      <td>1467844907</td>\n",
       "      <td>Mon Apr 06 22:28:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>brandonmcb</td>\n",
       "      <td>Living not downtown sure isn't much fun.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0</td>\n",
       "      <td>1467845095</td>\n",
       "      <td>Mon Apr 06 22:28:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mannyrique</td>\n",
       "      <td>@jonathanchard Not calorie wise   I wish junk ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0</td>\n",
       "      <td>1467845157</td>\n",
       "      <td>Mon Apr 06 22:28:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>styletrain</td>\n",
       "      <td>Man Work is Hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0</td>\n",
       "      <td>1467852031</td>\n",
       "      <td>Mon Apr 06 22:30:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kscud</td>\n",
       "      <td>getting sick  time for some hot tea, studying,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0</td>\n",
       "      <td>1467852067</td>\n",
       "      <td>Mon Apr 06 22:30:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kirstenj0y</td>\n",
       "      <td>Getting eyebrows waxed. More pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0</td>\n",
       "      <td>1467852789</td>\n",
       "      <td>Mon Apr 06 22:30:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>marculus</td>\n",
       "      <td>No phantasy star yesterday  going to work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0</td>\n",
       "      <td>1467853135</td>\n",
       "      <td>Mon Apr 06 22:30:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>andrewofthediaz</td>\n",
       "      <td>Oh - Just got all my MacHeist 3.0 apps - sweet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label         ids                          date      flag  \\\n",
       "150      0  1467844157  Mon Apr 06 22:28:32 PDT 2009  NO_QUERY   \n",
       "151      0  1467844505  Mon Apr 06 22:28:38 PDT 2009  NO_QUERY   \n",
       "152      0  1467844540  Mon Apr 06 22:28:38 PDT 2009  NO_QUERY   \n",
       "153      0  1467844907  Mon Apr 06 22:28:45 PDT 2009  NO_QUERY   \n",
       "154      0  1467845095  Mon Apr 06 22:28:48 PDT 2009  NO_QUERY   \n",
       "155      0  1467845157  Mon Apr 06 22:28:51 PDT 2009  NO_QUERY   \n",
       "156      0  1467852031  Mon Apr 06 22:30:34 PDT 2009  NO_QUERY   \n",
       "157      0  1467852067  Mon Apr 06 22:30:34 PDT 2009  NO_QUERY   \n",
       "158      0  1467852789  Mon Apr 06 22:30:45 PDT 2009  NO_QUERY   \n",
       "159      0  1467853135  Mon Apr 06 22:30:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                user                                              tweet  \n",
       "150         AKyarnie                      @onemoreproject that is lame   \n",
       "151       luimoral85              I don't understand... I really don't   \n",
       "152        ceironous  HEROES just isn't doing it for me this season...   \n",
       "153       brandonmcb         Living not downtown sure isn't much fun.    \n",
       "154       mannyrique  @jonathanchard Not calorie wise   I wish junk ...  \n",
       "155       styletrain                                  Man Work is Hard   \n",
       "156            kscud  getting sick  time for some hot tea, studying,...  \n",
       "157       kirstenj0y                 Getting eyebrows waxed. More pain   \n",
       "158         marculus       No phantasy star yesterday  going to work...  \n",
       "159  andrewofthediaz  Oh - Just got all my MacHeist 3.0 apps - sweet...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O que tem no dataset\n",
    "dataset.iloc[:][150:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af661416-95d2-453c-b6b9-f61e7349d638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   label   1600000 non-null  int64 \n",
      " 1   ids     1600000 non-null  int64 \n",
      " 2   date    1600000 non-null  object\n",
      " 3   flag    1600000 non-null  object\n",
      " 4   user    1600000 non-null  object\n",
      " 5   tweet   1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "253f4e96-496f-4c3d-9fc8-9abfc76d5773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substituir o 'label' 4 (positivo) para 1\n",
    "dataset[\"label\"].replace(4, 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d520b485-be8e-4a63-ad39-0633e446126b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.5\n",
       "1    0.5\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ver balanceamento de labels\n",
    "dataset[\"label\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595c40ec-c3f0-47d1-9976-071584fd42db",
   "metadata": {},
   "source": [
    "A amostra está balanceada. Não há problema em utilizar a **acurácia** para avaliar a classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b715be96-5b98-482d-ba54-6c39778e2655",
   "metadata": {},
   "source": [
    "Podemos dropar o Id, date (não precisamos avaliar em qual o momento foi twitado), flag (só tem um valor) e user (não precisamos saber quem disse). Além disso, não temos valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcbf9816-3dfb-4156-8ab8-2fee8d7dbd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop([\"ids\",\"date\",\"flag\",\"user\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4e7145d-57de-40b8-97b2-6d74d61d5ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet\n",
       "0      0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1      0  is upset that he can't update his Facebook by ...\n",
       "2      0  @Kenichan I dived many times for the ball. Man...\n",
       "3      0    my whole body feels itchy and like its on fire \n",
       "4      0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d893a6e0-b54d-47fb-8b54-c710031bae56",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a8bd3-0883-430c-a39c-7728532728f8",
   "metadata": {},
   "source": [
    "### Pré-processamento do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff0f59ff-2f3d-47d5-a6ba-0b7ed5245655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
    "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a172f480-f612-4b2f-bb1f-e9a510104031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Preprocessamento\n",
    "\n",
    "# Bibliotecas\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize  # tokenização\n",
    "from nltk.stem import *  # importar PorterStemmer() e WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords  # Stopwords\n",
    "import nltk  # para baixar os 'stopwords' e 'punkt' (punctuation)\n",
    "import re  # regex\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# Define as stopwords em inglês, português e espanhol\n",
    "#sw_english = set(stopwords.words('english'))\n",
    "\n",
    "# Instancia o PorterStemmer e WordNetLemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Função de preprocessamento - \n",
    "def preprocessing(string, method = 'stem'):\n",
    "    \"\"\"\n",
    "    Preprocessing for english\n",
    "    \"\"\"\n",
    "    # Substitui todas as URls com 'URL'\n",
    "    string = re.sub(r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\",' URL',string)\n",
    "    # Substitui todos os @USERNAME para 'USER'.\n",
    "    string = re.sub('@[^\\s]+',' USER', string)\n",
    "    # Manter somente caracteres e números - sem caracteres especiais\n",
    "    string = re.sub(r\"[^a-zA-Z0-9]+\", \" \", string)\n",
    "    # Letras minúsculas\n",
    "    string = string.lower()\n",
    "    # tokenização\n",
    "    words = word_tokenize(string)\n",
    "    \n",
    "    # sem stopwords em inglês e com palavras com mais de dois caracteres\n",
    "    filter_words = [word for word in words if word not in stopwordlist]\n",
    "    # Sem dropar as stopwords, aumenta-se a acurácia\n",
    "    lemmatized_words = []\n",
    "    \n",
    "    if method == 'stem':\n",
    "        for word in words:\n",
    "            sw = stemmer.stem(word)\n",
    "            lemmatized_words.append(sw)\n",
    "        return lemmatized_words\n",
    "    if method == 'lemma':\n",
    "        for word in words:\n",
    "            sw = lemmatizer.lemmatize(word)\n",
    "            lemmatized_words.append(sw)\n",
    "        return lemmatized_words\n",
    "    \n",
    "# use dataframe['nova_col'].apply(lambda x: preprocessing(x, \"stem\")) para criar uma nova coluna no dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9129334f-1cf7-4ce9-900f-9d2983e37c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando novas colunas no dataset para ML\n",
    "dataset['filtered_words'] = dataset[\"tweet\"].apply(lambda x: preprocessing(x, \"lemma\"))\n",
    "dataset['joined_words'] = dataset[\"filtered_words\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec1f06a0-5691-4585-bcb5-6472ed25f64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>filtered_words</th>\n",
       "      <th>joined_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[user, url, awww, that, s, a, bummer, you, sho...</td>\n",
       "      <td>user url awww that s a bummer you shoulda got ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[is, upset, that, he, can, t, update, his, fac...</td>\n",
       "      <td>is upset that he can t update his facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[user, i, dived, many, time, for, the, ball, m...</td>\n",
       "      <td>user i dived many time for the ball managed to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[my, whole, body, feel, itchy, and, like, it, ...</td>\n",
       "      <td>my whole body feel itchy and like it on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[user, no, it, s, not, behaving, at, all, i, m...</td>\n",
       "      <td>user no it s not behaving at all i m mad why a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet  \\\n",
       "0      0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1      0  is upset that he can't update his Facebook by ...   \n",
       "2      0  @Kenichan I dived many times for the ball. Man...   \n",
       "3      0    my whole body feels itchy and like its on fire    \n",
       "4      0  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                      filtered_words  \\\n",
       "0  [user, url, awww, that, s, a, bummer, you, sho...   \n",
       "1  [is, upset, that, he, can, t, update, his, fac...   \n",
       "2  [user, i, dived, many, time, for, the, ball, m...   \n",
       "3  [my, whole, body, feel, itchy, and, like, it, ...   \n",
       "4  [user, no, it, s, not, behaving, at, all, i, m...   \n",
       "\n",
       "                                        joined_words  \n",
       "0  user url awww that s a bummer you shoulda got ...  \n",
       "1  is upset that he can t update his facebook by ...  \n",
       "2  user i dived many time for the ball managed to...  \n",
       "3       my whole body feel itchy and like it on fire  \n",
       "4  user no it s not behaving at all i m mad why a...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8ceede-0d77-4abe-b226-d71a52db06dd",
   "metadata": {},
   "source": [
    "### Salvar dataset pré-processado\n",
    "path = \"../../twitter_proj/dataset_twitter.csv\"\n",
    "dataset.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d534d798-49b1-4f3a-adf4-c2a05a3e7f8e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bca87a-c3ae-4e62-b0a8-f11563624db7",
   "metadata": {},
   "source": [
    "### Palavras mais frequentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c502473-1869-4c79-8f0b-a3508cf023ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250bf876-d3c1-40df-8719-8e301af56e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_words = []\n",
    "for i in range(len(dataset)):\n",
    "    twitter_words += dataset.iloc[i][\"filtered_words\"]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a9433-adfe-4c60-8748-2417c8d4281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(twitter_words)\n",
    "fdist.plot(10, title = \"Palavras mais comuns no Twitter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9816ef90-a260-44dc-9a94-dfe70521e0a9",
   "metadata": {},
   "source": [
    "Nota-se que são palavras relativamente neutras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372ac04b-dbd4-4c0a-983d-5a6b2d164102",
   "metadata": {},
   "source": [
    "### Analisando as palavras mais comuns em tweets com sentimentos positivos e negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5dca89-2ad7-4677-b85f-be64f42828d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets com sentimentos positivos (1)\n",
    "twitter_words_pos = dataset[dataset[\"label\"] == 1]\n",
    "# Tweets com sentimentos negativos (0)\n",
    "twitter_words_neg = dataset[dataset[\"label\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb8329f-3523-4a3e-9989-8e39d9929153",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tweet_1 = []\n",
    "for i in range(len(twitter_words_pos[\"filtered_words\"])):\n",
    "    words_tweet_1 += twitter_words_pos.iloc[i][\"filtered_words\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98244ce7-de32-4752-8f5d-1697f2ba3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(words_tweet_1)\n",
    "fdist.plot(10, title = \"Palavras mais comuns no Twitter com sentimentos positivos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeb5f7a-ceb5-41d3-b434-d83e8c721cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tweet_0 = []\n",
    "for i in range(len(twitter_words_neg[\"filtered_words\"])):\n",
    "    words_tweet_0 += twitter_words_neg.iloc[i][\"filtered_words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f97af-665f-4e81-819f-8e2fa2d889e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(words_tweet_0)\n",
    "fdist.plot(10, title = \"Palavras mais comuns no Twitter com sentimentos negativos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e78e2a3-217b-49d8-b12c-4edcbe42398e",
   "metadata": {},
   "source": [
    "- Palavras relacionadas à sentimentos positivos: \"good\", \"love\", \"like\", \"lol\", \"thanks\".\n",
    "- Palavras relacionadas à sentimentos negativos: \"get\", \"work\", \"want\", \"going\", \"back\", \"miss\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9997db-6b82-4dfa-a24c-b2e3bde8af86",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12be9ed6-957f-4c37-963f-892f70e3cf2f",
   "metadata": {},
   "source": [
    "#### Dataset já pré-processado, tirado uma amostra de 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe904611-a6b7-454a-a735-6e547abcf02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset pré-processado\n",
    "path = \"../../twitter_proj/dataset_twitter.csv\"\n",
    "dataset = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1b0a1c5-9c6b-44dc-a46a-e31bb3c3d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como a amostra é muitp grande e fazer GridSearch é impensável, toma-se uma amostra de 30% - Foi feito com 10%, 20% e 30% e não houve mudanças significantes\n",
    "twitter_sample = dataset.sample(frac = 0.3, replace=False)\n",
    "twitter_sample.drop(columns = [\"Unnamed: 0\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6138428-805a-4b7f-9a39-a46513cbf37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>filtered_words</th>\n",
       "      <th>joined_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138709</th>\n",
       "      <td>0</td>\n",
       "      <td>@mjfh81 still at least you only have today to ...</td>\n",
       "      <td>['user', 'still', 'at', 'least', 'you', 'only'...</td>\n",
       "      <td>user still at least you only have today to go ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000243</th>\n",
       "      <td>1</td>\n",
       "      <td>@HouseofSpain thank you</td>\n",
       "      <td>['user', 'thank', 'you']</td>\n",
       "      <td>user thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165721</th>\n",
       "      <td>0</td>\n",
       "      <td>is very upset to see carys and donna go today ...</td>\n",
       "      <td>['is', 'very', 'upset', 'to', 'see', 'carys', ...</td>\n",
       "      <td>is very upset to see carys and donna go today ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290869</th>\n",
       "      <td>1</td>\n",
       "      <td>aw it's six months today  &amp;lt;3aab</td>\n",
       "      <td>['aw', 'it', 's', 'six', 'month', 'today', 'lt...</td>\n",
       "      <td>aw it s six month today lt 3aab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461578</th>\n",
       "      <td>0</td>\n",
       "      <td>@sgtmongoose samus has bio armor and a plasma ...</td>\n",
       "      <td>['user', 'samus', 'ha', 'bio', 'armor', 'and',...</td>\n",
       "      <td>user samus ha bio armor and a plasma cannon so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                              tweet  \\\n",
       "138709       0  @mjfh81 still at least you only have today to ...   \n",
       "1000243      1                           @HouseofSpain thank you    \n",
       "165721       0  is very upset to see carys and donna go today ...   \n",
       "1290869      1                 aw it's six months today  &lt;3aab   \n",
       "461578       0  @sgtmongoose samus has bio armor and a plasma ...   \n",
       "\n",
       "                                            filtered_words  \\\n",
       "138709   ['user', 'still', 'at', 'least', 'you', 'only'...   \n",
       "1000243                           ['user', 'thank', 'you']   \n",
       "165721   ['is', 'very', 'upset', 'to', 'see', 'carys', ...   \n",
       "1290869  ['aw', 'it', 's', 'six', 'month', 'today', 'lt...   \n",
       "461578   ['user', 'samus', 'ha', 'bio', 'armor', 'and',...   \n",
       "\n",
       "                                              joined_words  \n",
       "138709   user still at least you only have today to go ...  \n",
       "1000243                                     user thank you  \n",
       "165721   is very upset to see carys and donna go today ...  \n",
       "1290869                    aw it s six month today lt 3aab  \n",
       "461578   user samus ha bio armor and a plasma cannon so...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9ae0c3a-0d2f-4a00-a786-f340bcb021f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.500958\n",
       "0    0.499042\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ver balanceamento de labels\n",
    "twitter_sample[\"label\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "403364ad-cedb-4d76-b510-06b92278f6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user thank you'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_sample.iloc[1,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98028bfd-b924-43e6-b77b-2e410fba9b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count    Dtype \n",
      "---  ------          --------------    ----- \n",
      " 0   Unnamed: 0      1600000 non-null  int64 \n",
      " 1   label           1600000 non-null  int64 \n",
      " 2   tweet           1600000 non-null  object\n",
      " 3   filtered_words  1600000 non-null  object\n",
      " 4   joined_words    1600000 non-null  object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 61.0+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bff8c5-20fa-4e4d-b6fb-3456218d12f5",
   "metadata": {},
   "source": [
    "## Modelos de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf9200d3-48f5-470d-8f45-b5f49068840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas e modelos\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Processamentos\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Usar os modelos abaixo\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc2df7e-d2ad-4a54-886d-6f8165a60927",
   "metadata": {},
   "source": [
    "### Baseline com LogisticRegression (sem otimização de hiperparâmetros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e4f4f37-3df7-4eeb-965e-56eeaf2beb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar amostras de treino e teste\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataset[\"joined_words\"]\n",
    "y = dataset[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53b95845-362a-48fe-a8a4-7372162c92d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('model', LogisticRegression())])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vetorização e modelo de ML\n",
    "text_model_lr = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"model\", LogisticRegression())\n",
    "])\n",
    "\n",
    "text_model_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f49d5535-76a1-4ad9-a19f-98426fef98b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "text_model_lr.fit(X_train, y_train)\n",
    "predictions_lr = text_model_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f934f0b8-a0a0-4226-bd22-4a0823bc13d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[188637  50724]\n",
      " [ 46116 194523]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.80    239361\n",
      "           1       0.79      0.81      0.80    240639\n",
      "\n",
      "    accuracy                           0.80    480000\n",
      "   macro avg       0.80      0.80      0.80    480000\n",
      "weighted avg       0.80      0.80      0.80    480000\n",
      "\n",
      "\n",
      "A acurácia é 0.79825\n",
      "A roc auc score é 0.7982230096218319\n",
      "A f1 é 0.8006939899482595\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, roc_auc_score, f1_score\n",
    "\n",
    "print(confusion_matrix(y_test, predictions_lr))\n",
    "print()\n",
    "print(classification_report(y_test, predictions_lr))\n",
    "print()\n",
    "print(f\"A acurácia é {accuracy_score(y_test, predictions_lr)}\")\n",
    "print(f\"A roc auc score é {roc_auc_score(y_test, predictions_lr)}\")\n",
    "print(f\"A f1 é {f1_score(y_test, predictions_lr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d51e29-62d5-42ad-8b37-037ca20eb0e8",
   "metadata": {},
   "source": [
    "Usando os dados como um todo, temos com a baseline uma acurácia de 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe3aba-1c50-4b59-84d0-d91544f5f9cf",
   "metadata": {},
   "source": [
    "### LogisticRegression com tunning de Hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf64e3e0-4388-482c-b946-1472ed8833e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar amostras de treino e teste\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = twitter_sample[\"joined_words\"]\n",
    "y = twitter_sample[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "171fd2fd-591f-49e9-8fa9-d682db18ced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vetorização e modelo de ML\n",
    "text_model_lrh = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"model\", LogisticRegression())\n",
    "])\n",
    "\n",
    "# param_grid\n",
    "param_grid={'model__penalty': ['l1', 'l2'],\n",
    "            'model__solver': ['liblinear']}\n",
    "\n",
    "# GridSearch\n",
    "fold = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 42)\n",
    "grid_lr = GridSearchCV(text_model_lrh, param_grid = param_grid, cv = fold, scoring = 'neg_mean_absolute_error', return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cc04b8f-9d6a-43ba-9457-418accc55289",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lr.fit(X_train, y_train)\n",
    "predictions = grid_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4ec79c8-2ffd-4286-9016-10fda875ac84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model__penalty': 'l2', 'model__solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "# Melhores parâmetros\n",
    "print(grid_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18801332-6915-4909-8e50-316b5974603b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56153 15398]\n",
      " [14276 58173]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79     71551\n",
      "           1       0.79      0.80      0.80     72449\n",
      "\n",
      "    accuracy                           0.79    144000\n",
      "   macro avg       0.79      0.79      0.79    144000\n",
      "weighted avg       0.79      0.79      0.79    144000\n",
      "\n",
      "\n",
      "A acurácia é 0.7939305555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, roc_auc_score\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print()\n",
    "print(classification_report(y_test, predictions))\n",
    "print()\n",
    "print(f\"A acurácia é {accuracy_score(y_test, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe38af3-0de2-49eb-9fae-f33dcc0c7c21",
   "metadata": {},
   "source": [
    "Não houve muita mudança com tunning de hiperparâmetros. Com 30% dos dados a acurácia vai para 79.39%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d700a13-befe-4ff5-824a-2da01134d3e0",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3d8a632-c4a0-4ba7-b367-6b5581a401c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('model', RandomForestClassifier())])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo RandomForest\n",
    "text_model_rf = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('model', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "text_model_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08349fcb-765e-44f7-b87b-813da99b2a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model_rf.fit(X_train, y_train)\n",
    "predictions = text_model_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882d314-9724-44ba-89fd-1c77e48931a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melhores parâmetros\n",
    "print(text_model_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca283eb9-0f49-4cd3-bb8d-0705706f8f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, roc_auc_score\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print()\n",
    "print(classification_report(y_test, predictions))\n",
    "print()\n",
    "print(f\"A acurácia é {accuracy_score(y_test, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d394b59-5ba5-4a4e-bfe9-ab9dd6f806b5",
   "metadata": {},
   "source": [
    "A acurácia ficou menor que em LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2562538-da5a-4537-baa2-2d076e3e0a00",
   "metadata": {},
   "source": [
    "### RandomForest com tunning de hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be93484c-a2f2-4378-9908-787721575943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo RandomForest\n",
    "text_model_rfh = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('model', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Param_grid\n",
    "param_grid={'model__max_depth': [10, 15, 20],\n",
    "            'model__criterion': ['entropy', 'gini']}\n",
    "\n",
    "# GridSearch\n",
    "fold = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 42)\n",
    "grid_rfh = GridSearchCV(text_model_rfh, param_grid = param_grid, cv = fold, scoring = 'neg_mean_absolute_error', return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aad010ad-514a-4217-943f-9744e56ca3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rfh.fit(X_train, y_train)\n",
    "predictions = grid_rfh.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65f53e85-6d29-4761-b5b1-fb0c9f81ce9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model__criterion': 'entropy', 'model__max_depth': 20}\n"
     ]
    }
   ],
   "source": [
    "# Melhores parâmetros\n",
    "print(grid_rfh.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "968322ad-4ea4-4d04-8886-17551d46f7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50676 21317]\n",
      " [16219 55788]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.70      0.73     71993\n",
      "           1       0.72      0.77      0.75     72007\n",
      "\n",
      "    accuracy                           0.74    144000\n",
      "   macro avg       0.74      0.74      0.74    144000\n",
      "weighted avg       0.74      0.74      0.74    144000\n",
      "\n",
      "\n",
      "A acurácia é 0.7393333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, roc_auc_score\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print()\n",
    "print(classification_report(y_test, predictions))\n",
    "print()\n",
    "print(f\"A acurácia é {accuracy_score(y_test, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa1518-9118-4e02-89a1-54e9fe54580c",
   "metadata": {},
   "source": [
    "### RandomForest com PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3486dc0a-d47e-4e1c-b54f-cc91f4c0af0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('sc', StandardScaler(with_mean=False)),\n",
       "                ('svd', TruncatedSVD(n_components=50, random_state=42)),\n",
       "                ('rf', RandomForestClassifier())])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo com SVD\n",
    "text_model_rfsvd = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    (\"sc\", StandardScaler(with_mean=False)),\n",
    "    (\"svd\", TruncatedSVD(n_components = 50, random_state = 42)),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "text_model_rfsvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f6fef0f-be4c-43c7-9046-8463b0447520",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model_rfsvd.fit(X_train, y_train)\n",
    "predictions = text_model_rfsvd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49ae9f50-74a5-4a2a-8ce4-10ed65ed740f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46482 25511]\n",
      " [29712 42295]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.65      0.63     71993\n",
      "           1       0.62      0.59      0.61     72007\n",
      "\n",
      "    accuracy                           0.62    144000\n",
      "   macro avg       0.62      0.62      0.62    144000\n",
      "weighted avg       0.62      0.62      0.62    144000\n",
      "\n",
      "\n",
      "A acurácia é 0.6165069444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, roc_auc_score\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print()\n",
    "print(classification_report(y_test, predictions))\n",
    "print()\n",
    "print(f\"A acurácia é {accuracy_score(y_test, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515224fe-e4a7-499e-ae62-c3730fee5fc9",
   "metadata": {},
   "source": [
    ": (\n",
    "\n",
    "é preciso utilizar mais componentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ebd2f-4a73-49dc-9e07-b918ec9dccf2",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa173358-027d-462d-8ee7-0a8e8a0a1a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('model', LinearSVC())])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model_lsvc = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"model\", LinearSVC())\n",
    "])\n",
    "\n",
    "text_model_lsvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6794f6f-0f06-4777-8952-cb7f1e43895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model_lsvc.fit(X_train, y_train)\n",
    "predictions = text_model_lsvc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0554fa8f-adfe-4ba0-ad07-075ceb14920b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55692 16301]\n",
      " [14907 57100]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.77      0.78     71993\n",
      "           1       0.78      0.79      0.79     72007\n",
      "\n",
      "    accuracy                           0.78    144000\n",
      "   macro avg       0.78      0.78      0.78    144000\n",
      "weighted avg       0.78      0.78      0.78    144000\n",
      "\n",
      "\n",
      "A acurácia é 0.7832777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, roc_auc_score\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print()\n",
    "print(classification_report(y_test, predictions))\n",
    "print()\n",
    "print(f\"A acurácia é {accuracy_score(y_test, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f6d698-ae7b-4211-aa6c-fa4c08b1a91a",
   "metadata": {},
   "source": [
    "### Linear SVC com tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "217839aa-602a-414e-8f74-e7e9cd29130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model_svc = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"model\", LinearSVC())\n",
    "])\n",
    "\n",
    "param_grid={\n",
    "            'model__C': [1.0, 10.0, 100.0]}\n",
    "\n",
    "# GridSearch\n",
    "fold = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 42)\n",
    "grid_svc = GridSearchCV(text_model_svc, param_grid = param_grid, cv = fold, scoring = 'neg_mean_absolute_error', return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "889bdc9e-85d8-4208-91b7-53991a889820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "grid_svc.fit(X_train, y_train)\n",
    "predictions = grid_svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc9151bf-1117-4b1c-ad9a-69412a02c0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55692 16301]\n",
      " [14907 57100]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.77      0.78     71993\n",
      "           1       0.78      0.79      0.79     72007\n",
      "\n",
      "    accuracy                           0.78    144000\n",
      "   macro avg       0.78      0.78      0.78    144000\n",
      "weighted avg       0.78      0.78      0.78    144000\n",
      "\n",
      "\n",
      "A acurácia é 0.7832777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, roc_auc_score\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print()\n",
    "print(classification_report(y_test, predictions))\n",
    "print()\n",
    "print(f\"A acurácia é {accuracy_score(y_test, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f129477-ffcd-457e-9e74-b36e0877e8cc",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "019b4be3-d283-45a4-a1c5-7f8d0602d393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('model',\n",
       "                 XGBClassifier(base_score=None, booster=None,\n",
       "                               colsample_bylevel=None, colsample_bynode=None,\n",
       "                               colsample_bytree=None, eval_metric='logloss',\n",
       "                               gamma=None, gpu_id=None, importance_type='gain',\n",
       "                               interaction_constraints=None, learning_rate=0.1,\n",
       "                               max_delta_step=None, max_depth=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, n_estimators=100,\n",
       "                               n_jobs=None, num_parallel_tree=None,\n",
       "                               random_state=None, reg_alpha=None,\n",
       "                               reg_lambda=None, scale_pos_weight=None,\n",
       "                               subsample=None, tree_method=None,\n",
       "                               validate_parameters=None, verbosity=None))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model_xgb = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"model\", XGBClassifier(learning_rate = 0.1, eval_metric = \"logloss\"))\n",
    "])\n",
    "\n",
    "text_model_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69cf4876-d9f8-4160-8817-a19cacd406ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "text_model_xgb.fit(X_train, y_train)\n",
    "predictions = text_model_xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75df9b9d-cdd8-4668-9cfe-37c45b535d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[53084 18909]\n",
      " [20764 51243]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73     71993\n",
      "           1       0.73      0.71      0.72     72007\n",
      "\n",
      "    accuracy                           0.72    144000\n",
      "   macro avg       0.72      0.72      0.72    144000\n",
      "weighted avg       0.72      0.72      0.72    144000\n",
      "\n",
      "\n",
      "A acurácia é 0.7244930555555555\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, roc_auc_score\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print()\n",
    "print(classification_report(y_test, predictions))\n",
    "print()\n",
    "print(f\"A acurácia é {accuracy_score(y_test, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff7bfa0-3638-45b7-8be4-b288784e7516",
   "metadata": {},
   "source": [
    "Ficou pior!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22180615-1777-4c18-a89e-a67bd90c76ca",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b032ca4-0c83-482d-8598-3bfdab0ce5bc",
   "metadata": {},
   "source": [
    "#### LogisticRegression com GridSearch teve maior acurácia, 79.39% com 30% dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647a162f-8224-4c5c-956a-a679815c51a0",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
