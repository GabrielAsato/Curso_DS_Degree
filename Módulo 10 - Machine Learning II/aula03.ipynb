{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "Chegamos ao nosso último método de ensemble, o XGBoost (e**X**treme **G**radient **Boost**ing).\n",
    "\n",
    "Este método nada mais é que um gradient boosting, mas com algumas importantes modificações que lhe conferem o título de \"extreme\"! Em particular, duas alterações merecem destaque:\n",
    "\n",
    "- A adição de procedimentos de regularização (L1 e L2!), o que melhora consideravelmente sua capacidade de generalização;\n",
    "\n",
    "- A utilização de derivadas de segunda ordem (Hessiano) para o procedimento de gradiente.\n",
    "\n",
    "Para quem quiser se aventurar mais, sugiro algumas boas leituras:\n",
    "\n",
    "- [Este](https://shirinsplayground.netlify.app/2018/11/ml_basics_gbm/), explica bem as particularidades do XGBoost, além de dar uma boa introdução ao gradient boosting (o código é em R, então pode ignorar essa parte hehe);\n",
    "\n",
    "- [Este](https://medium.com/analytics-vidhya/what-makes-xgboost-so-extreme-e1544a4433bb), introduz bem o método, enquanto enfativa suas particularidades, com alguns detalhes matemáticos;\n",
    "\n",
    "- [Este](https://xgboost.readthedocs.io/en/latest/tutorials/model.html), da própria documentação da biblioteca, traz uma explicação legal, e com alguns detalhes matemáticos;\n",
    "\n",
    "- [Este](https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d), com uma discussão mais alto-nível (sem tantos detalhes) sobre o XGBoost e os motivos de seu sucesso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basicamente temos um GradientBoost Extreme:\n",
    "- Quanto ao paralelismo, a construção sequêncial de arvores é paralelisada.\n",
    "    - Temos o loop externo que enumera os nós folha de uma árvore e o segundo loop interno que calcula os recursos, o truque é primeiro preparar várias linhas de execução para cada folha e cada uma delas é disparada no momento do cálculo da hipotese.\n",
    "- Poda da árvore: Após a expansão da árvore, pode ocorrer que os subconjuntos de nós folhas sejam muito pequenos e com grande efeito de overfitting e esse ajuste pode ser minizado, já que temos o critério de max_depth.\n",
    "- Otimização de hardware.\n",
    "- Já regulariza por Lasso e Ridge.\n",
    "- Tratamento de valores faltantes, já \"aprende\" qual o melhor valor para adotar\n",
    "- Aplica quantil ponderado nos dados.\n",
    "- Validação cruzada integrado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://cdn-images-1.medium.com/max/1000/1*U72CpSTnJ-XTjCisJqCqLg.jpeg' width=600 height=1200/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns hiperparâmetros: ( https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters )\n",
    "- learning_rate: taxa de aprendizado, valores menores de passo usado para evitar overfitting. O intervalo é [0,1].\n",
    "- max_depth: determina quão profundamente cada árvore pode crescer durante qualquer rodada de reforço.\n",
    "- subsample: porcentagem de amostras utilizadas por árvore. O baixo valor pode levar ao underfitting.\n",
    "- colsample_bytree: porcentagem de recursos usados por árvore. Alto valor pode levar a overfitting.\n",
    "- n_estimators: número de árvores que você deseja construir.\n",
    "- objective: determina a função de perda a ser usada como reg:linear para problemas de regressão, reg:logistic para problemas de classificação com apenas decisão, binary:logistic para problemas de classificação com probabilidade.\n",
    "- Regularização\n",
    "    - gamma: controla se um determinado nó será dividido com base na redução esperada na perda após a divisão. Um valor mais alto leva a menos divisões. Suportado apenas para alunos baseados em árvore.\n",
    "    - alpha: Regularização L1 nos pesos das folhas. Um valor grande leva a mais regularização.\n",
    "    - lambda: Regularização L2 nos pesos das folhas e é mais suave do que a regularização L1.\n",
    "- Validação Cruzada\n",
    "    - num_boost_round: denota o número de árvores que você constrói (análogo a n_estimators)\n",
    "    - metrics: informa as métricas de avaliação a serem observadas durante o CV\n",
    "    - early_stopping_rounds: termina o treinamento do modelo antecipadamente se a métrica de hold-out (\"rmse\" em nosso caso) não melhorar em um determinado número de rodadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infelizmente, o sklearn não tem o XGBoost implementado :(\n",
    "\n",
    "Mas, felizmente, existe uma biblioteca que o implementou, de maneira totalmente integrada ao sklearn!!\n",
    "\n",
    "A biblioteca é a [XGBoost](https://xgboost.readthedocs.io/en/latest/).\n",
    "\n",
    "Para instalar a biblioteca, o de sempre: ( https://xgboost.readthedocs.io/en/latest/install.html#python )\n",
    "\n",
    "`!pip install xgboost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\gabri\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from xgboost) (1.20.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from xgboost) (1.6.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for retrying: [Errno 2] No such file or directory: 'c:\\\\users\\\\gabri\\\\anaconda3\\\\lib\\\\site-packages\\\\retrying-1.3.3.dist-info\\\\METADATA'\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_breast_cancer(as_frame = True)[\"frame\"]\n",
    "df.head()\n",
    "\n",
    "y_colum = 'target'\n",
    "\n",
    "X = df.drop(columns=[y_colum])\n",
    "y = df[y_colum]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      " 30  target                   569 non-null    int32  \n",
      "dtypes: float64(30), int32(1)\n",
      "memory usage: 135.7 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe_preprocessor(path_dataset, y_colum):\n",
    "    df = pd.read_csv(path_dataset, index_col=0)\n",
    "\n",
    "    X = df.drop(columns=[y_colum])\n",
    "    y = df[y_colum]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    pipe_features_num = Pipeline([\n",
    "        ('input_num', SimpleImputer(strategy='mean')),\n",
    "        ('std', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    features_num = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "    \n",
    "    pipe_features_cat = Pipeline([\n",
    "        ('input_cat', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "        ('ondehot', OneHotEncoder())\n",
    "    ])\n",
    "    \n",
    "    features_cat = X_train.select_dtypes(exclude=np.number).columns.tolist()\n",
    "    \n",
    "    pre_processor = ColumnTransformer([\n",
    "        ('transf_num', pipe_features_num, features_num),\n",
    "        ('transf_cat', pipe_features_cat, features_cat)\n",
    "    ])\n",
    "    \n",
    "    return pre_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metricas_classificacao(estimator):\n",
    "    \n",
    "    # ============================================\n",
    "\n",
    "    print(\"\\nMétricas de avaliação de treino:\")\n",
    "\n",
    "    y_pred_train = estimator.predict(X_train)\n",
    "\n",
    "    print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "\n",
    "    # ============================================\n",
    "\n",
    "    print(\"\\nMétricas de avaliação de teste:\")\n",
    "\n",
    "    y_pred_test = estimator.predict(X_test)\n",
    "\n",
    "\n",
    "    print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:11:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "metricas_classificacao() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-d943fd68cbf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmetricas_classificacao\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: metricas_classificacao() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "metricas_classificacao(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentando um pouco a regularização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-22T00:39:39.568774Z",
     "start_time": "2022-02-22T00:39:38.598799Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Boosting e estratégias evolucionárias na tarefa de regressao para a mineraçao de dados temporais\n",
    "    - https://acervodigital.ufpr.br/handle/1884/3770?show=full\n",
    "- XGBoosting visão geral\n",
    "    - https://www.kdnuggets.com/2019/05/xgboost-algorithm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercício\n",
    "Leia o material https://towardsdatascience.com/xgboost-fine-tune-and-optimize-your-model-23d996fab663  \n",
    "Pratique o utilizar um GridSearch para ajudar os hiperparâmetros e relate um ajuste que melhorou o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
