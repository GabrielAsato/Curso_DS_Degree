{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e0516d",
   "metadata": {},
   "source": [
    "# Preparação para a Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb94c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "pergunta = \"\"\"\n",
    "{} - Pensando no algoritimo de Machine Learning \\\"{}\\\" responda:\n",
    "a) Resumidamente como ele funciona?\n",
    "b) Quais são suas forças, o que o difere dos outros algoritmos?\n",
    "c) E suas fraquezas?\n",
    "d) Como é seu método de avaliação?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a328fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nomes_algoritmos = [\n",
    "    'SVM',\n",
    "    'SVR',\n",
    "    'AdaBoosting',\n",
    "    'GradientBooting',\n",
    "    'XGBoosting',\n",
    "    'K-Means',\n",
    "    'DBScan',\n",
    "    'Agglomerative Clustering'\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a9514a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 - Pensando no algoritimo de Machine Learning \"SVM\" responda:\n",
      "a) Resumidamente como ele funciona?\n",
      "b) Quais são suas forças, o que o difere dos outros algoritmos?\n",
      "c) E suas fraquezas?\n",
      "d) Como é seu método de avaliação?\n",
      "\n",
      "\n",
      "2 - Pensando no algoritimo de Machine Learning \"SVR\" responda:\n",
      "a) Resumidamente como ele funciona?\n",
      "b) Quais são suas forças, o que o difere dos outros algoritmos?\n",
      "c) E suas fraquezas?\n",
      "d) Como é seu método de avaliação?\n",
      "\n",
      "\n",
      "3 - Pensando no algoritimo de Machine Learning \"AdaBoosting\" responda:\n",
      "a) Resumidamente como ele funciona?\n",
      "b) Quais são suas forças, o que o difere dos outros algoritmos?\n",
      "c) E suas fraquezas?\n",
      "d) Como é seu método de avaliação?\n",
      "\n",
      "\n",
      "4 - Pensando no algoritimo de Machine Learning \"GradientBooting\" responda:\n",
      "a) Resumidamente como ele funciona?\n",
      "b) Quais são suas forças, o que o difere dos outros algoritmos?\n",
      "c) E suas fraquezas?\n",
      "d) Como é seu método de avaliação?\n",
      "\n",
      "\n",
      "5 - Pensando no algoritimo de Machine Learning \"XGBoosting\" responda:\n",
      "a) Resumidamente como ele funciona?\n",
      "b) Quais são suas forças, o que o difere dos outros algoritmos?\n",
      "c) E suas fraquezas?\n",
      "d) Como é seu método de avaliação?\n",
      "\n",
      "\n",
      "6 - Pensando no algoritimo de Machine Learning \"K-Means\" responda:\n",
      "a) Resumidamente como ele funciona?\n",
      "b) Quais são suas forças, o que o difere dos outros algoritmos?\n",
      "c) E suas fraquezas?\n",
      "d) Como é seu método de avaliação?\n",
      "\n",
      "\n",
      "7 - Pensando no algoritimo de Machine Learning \"DBScan\" responda:\n",
      "a) Resumidamente como ele funciona?\n",
      "b) Quais são suas forças, o que o difere dos outros algoritmos?\n",
      "c) E suas fraquezas?\n",
      "d) Como é seu método de avaliação?\n",
      "\n",
      "\n",
      "8 - Pensando no algoritimo de Machine Learning \"Agglomerative Clustering\" responda:\n",
      "a) Resumidamente como ele funciona?\n",
      "b) Quais são suas forças, o que o difere dos outros algoritmos?\n",
      "c) E suas fraquezas?\n",
      "d) Como é seu método de avaliação?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, nome_algoritmo in enumerate(nomes_algoritmos, start=1):\n",
    "    print(pergunta.format(i, nome_algoritmo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3055f04",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1356811d",
   "metadata": {},
   "source": [
    "1 - Pensando no algoritimo de Machine Learning \"SVM\" responda:\n",
    "\n",
    "a) **Resumidamente como ele funciona?** Support Vector Machine (SVM) é um algoritmo de aprendizado supervisionado muito usado para classificação, mas pode ser usado para regressão. A ideia principal é, baseado nos dados de treino, tenta achar o **hiperplano ótimo** na qual pode usar para classificar novos pontos de dados (test dataset). Em uma dimensão, o **hiperplano** é um ponto, e em duas dimensões é uma linha. O hiperplano são limites de decisão que classificam o conjunto de dados enquanto maximizam a margem.   \n",
    "    A função que classifca é chamada de **kernel**, que calcula basicamente a distância entre duas observações. O SVM localiza esse \"hiperplano\" para maximizar a margem (largura ou espessura do hiperplano, ou a distância normal entre os vetores de suporte e o hiperplano) entre os vetores de suporte (pontos mais próximos ao hiperplano) das duas classes.\n",
    "    O melhor hiperplano capaz de separar duas classes estaria localizado no ponto médio entre eles (maximizando a margem), trazendo uma certa característica de simetria ma classificação, onde o ponto mais próximo de cada classe está a uma distância d (meia margem) do hiperplano, de forma a minimizar erros de classificação e problemas de enviesamento (bias) do modelo (overfitting).\n",
    "    Quando os dadods não são linearmente separáveis, não podemos desenhar uma linha reta para separar as duas classes. Para resolver o problema, usamos dados não linearmente separáveis no espaço n-dimensional. Transforme-o em um espaço dimensional mais alto para torná-lo linearmente separável. \n",
    "\n",
    "b) **Quais são suas forças, o que o difere dos outros algoritmos?**\n",
    "    - Trabalha bem em espaços de alta dimensão\n",
    "    - Eficaz nos casos em que o número de dimensões é maior que o número de amostras\n",
    "    - Sempre busca o mínimo global\n",
    "    - Utilizado para dados separáveis linearmente e não linearmente separáveis\n",
    "    - é eficiente porque utiliza apenas os dados dos vetores de suporte\n",
    "    - Usado para detectar outliers\n",
    "\n",
    "c) **E suas fraquezas?**\n",
    "    - Tempo de treinamento é alto se existe muitos dados\n",
    "    - Interpretação e visualização do modelo é extremamente complexa\n",
    "    - Não funciona muito bem quando o conjunto de dados tem mais ruído, ou seja, as classes de destino estão sobrepostas.\n",
    "    - Escolhas indevidas de hiperparâmetros **C** e **Gamma** podem levar a overfitting (Usar RandomSearchCV ou GridSearchCV)\n",
    "\n",
    "d) **Como é seu método de avaliação?**\n",
    "    - Utilizamos métodos de avaliação de classificação\n",
    "        - Classification report\n",
    "            - Precision\n",
    "            - Recall\n",
    "            - F1 score\n",
    "            - Accuracy\n",
    "        - Confusion matrix\n",
    "\n",
    "--\n",
    "- https://medium.com/@msremigio/m%C3%A1quinas-de-vetores-de-suporte-svm-77bb114d02fc\n",
    "- https://www.linkedin.com/pulse/introdu%C3%A7%C3%A3o-ao-svm-como-classificador-rodrigo-araujo-\n",
    "- https://towardsdatascience.com/svm-and-kernel-svm-fed02bef1200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337cf5d8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ae655",
   "metadata": {},
   "source": [
    "2 - Pensando no algoritimo de Machine Learning \"SVR\" responda:\n",
    "\n",
    "a) **Resumidamente como ele funciona?**\n",
    "    Da mesma forma vista em SVM, os elementos de classificação também são relevantes aqui. A utilização do kernel para que um modelo de regressão linear seja treinado no espaço de features. No espaço, de inputs, este modelo é refletido como uma regressão **não-linear**. A principal diferença é que o conceito de margem também está presente, de modo que apenas alguns pontos efetivamente vão construir para regressão. Neste caso, são pontos dentro da margem (região conhecida como e-tubo) que serão estes vetores de suporte, ou seja, os pontos fora da margem não contribuem para a função custo.\n",
    "     SVR fornece uma flexibilidade em definir o quanto os erros são aceitáveis no modelo e encontrar o hiperplano apropriado que fita os dados. Em comparação com OLS, a Função Objetiva do SVR é minimizar os coeficientes -mais especificamente, a norma L2 do vetor de coeficientes- não o erro quadrado. O erro é analisado na restrição, onde calculamos o erro absoluto seja menor ou igual a margem, chamado de máximo epslon. Podemos tunar o epslon para atingir a acurácia do modelo.\n",
    "    Em outras palavras, minimizar:\n",
    "             $$min \\frac{1}{2}*\\|w\\|^2$$\n",
    "    E restringir:\n",
    "            $$|y_i-w_i*x_i|\\leq\\epsilon$$\n",
    "        \n",
    "b) **Quais são suas forças, o que o difere dos outros algoritmos?**\n",
    "    - é robusto contra outliers\n",
    "    - O modelo decisão pode ser atualizado facilmente\n",
    "    - Tem uma excelente capacidade de generalização, com grande acurácia\n",
    "    - Facilmente implementado\n",
    "    \n",
    "c) **E suas fraquezas?**\n",
    "    - Não é recomendado para datasets grandes\n",
    "    - Em um caso onde o número de features excede o número de dados, o SVM não performará bem.\n",
    "    - Assim como em SVM, não performa bem com ruído, ou seja, dados sobrepostos.\n",
    "    \n",
    "d) **Como é seu método de avaliação?**\n",
    "    - Mesma métricas de regressão como r2_score\n",
    "\n",
    "--\n",
    "- https://towardsdatascience.com/an-introduction-to-support-vector-regression-svr-a3ebc1672c2\n",
    "- https://towardsdatascience.com/unlocking-the-true-power-of-support-vector-regression-847fd123a4a0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad695c42",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdfb8f2",
   "metadata": {},
   "source": [
    "3 - Pensando no algoritimo de Machine Learning \"AdaBoosting\" responda:\n",
    "\n",
    "a) **Resumidamente como ele funciona?**\n",
    "    O Adaboost significa Adaptative Boosting e tem como procedimento geral a criação de sucessiva dos chamados \"weak learners\" que são modelos fracos de aprendizagem (no caso de árovores decisão, são stumps). O Adaboost utiliza os erros da árvore anterior para melhorar a próxima árvore. As predições finais são feitas com base nos **pesos de cada stump**. O método começa treinando um classificador fraco no **dataset original**, e depois treina diversas cópias adicionais do classificador **no mesmo dataset**, mas dando **um peso maior às observações que foram classificadas erroneamente** (nos casos de regressão, as observações com maior erro).\n",
    "    Inicialmente, todas as amostras tem pesos iguais. A medida que o algoritmo avança, os pesos das amostras classificadas incorretamente são incrementadas. Um stump com uma feature é construído e calcula-se o erro total (ou seja, a soma dos pesos classifcadas erradas). Em seguida, calcula-se o novo peso (alfa) como:\n",
    "    $$\\alpha = \\eta * \\frac{1-erroTotal}{erroTotal}$$\n",
    "    Onde\n",
    "    $$erroTotal = \\sum{pesosErrados}$$\n",
    "    Em seguida, atualizam os pesos:\n",
    "    $$novopeso = pesoAnterior * e^\\alpha$$\n",
    "    se classificou de maneira errada e\n",
    "    $$novopeso = pesoAnterior * e^{-\\alpha}$$\n",
    "    se classificou de maneira correta\n",
    "    Ou seja, aumenta-se o peso para classificação errada e diminui para as certas. Outro fator é que a soma dos pesos devem somar 1, por isso, **precisa ser normalizado**.\n",
    "    Um novo dataset é montado e aqueles com maiores pesos tem mais chance de surgirem nesse dataset. Os passos de criar o stump até a criação de um novo dataset com maiores pesos é repetido até o número de iterações do hiperparâmetro.\n",
    "    \n",
    "b) **Quais são suas forças, o que o difere dos outros algoritmos?**\n",
    "    - Pode ser usado tanto para classificaçõs como para regressões\n",
    "    - Lida bem com dados diferentes\n",
    "    - Modelo resistente a outliers caso usada a Função de Perda apropriada\n",
    "    \n",
    "c) **E suas fraquezas?**\n",
    "    - Não pode ser paralelizado (não é escalável)\n",
    "    - Risco de overfitting\n",
    "    - Ajuste de parâmetros é mais difícil\n",
    "    \n",
    "d) **Como é seu método de avaliação?**\n",
    "    Mesmos métodos de avaliação para Classificação\n",
    "\n",
    "--\n",
    "https://pedroazambuja.medium.com/adaboost-adaptive-boosting-dbbec150fced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3ee90",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f3052",
   "metadata": {},
   "source": [
    "4 - Pensando no algoritimo de Machine Learning \"GradientBooting\" responda:\n",
    "\n",
    "a) **Resumidamente como ele funciona?**\n",
    "    O gradient boosting também é baseado no método de boosting (usar weak learners em sequência para minimizar os erros cometidos). Neste caso, o boosting é implementado através de um **gradiente** explícito. A ideia é caminhar na direção do erro mínimo de maneira iterativa. Esse caminho se dá pelo **gradiente da função de custo/perda**, que mede os erros cometidos.\n",
    "    O objetivo geral do método é: determinar quais são os **parâmetros** da hipótese que minimizam a função custo/perda.\n",
    "    A função custo/perda (*loss*) é explicitamente minimizada por um procedimento de gradiente. E o gradiente está relacionado com o procedimento de **encadeamento progressivo de weak learners**.\n",
    "    Os principais hiperparâmetros são: n_etimators (número de weak learners) e learning_rate (constante que multiplica o gradiente descendente (~0.1)).\n",
    "    O algoritmo funciona do seguinte modo:\n",
    "- um valor $F_0(x)$ é criado a partir da minimização da função de custo/perda.\n",
    "- Em seguida, o residuo ($r_n$) é calculado a partir da derivada da função custo/perda em relação ao valor predito\n",
    "- Esse valor é computado.\n",
    "- Ao criar um stump, colocar em cada lado o resíduo de acordo com a decisão da árvore.\n",
    "- Calcular $\\gamma_{ij}$ que minimiza os valores de $R_{ij}= [r_1, r_2, ...]$\n",
    "- Atualizar o valor de $F_M(x) = F_{M-1}(x) + \\eta * \\sum{\\gamma_{ij}}$\n",
    "        \n",
    "b) **Quais são suas forças, o que o difere dos outros algoritmos?**\n",
    "\n",
    "- ótima precisão de previsão\n",
    "- Otimização com diferentes funções de perda (precisa ser diferenciável) e vários ajustes de hiperparâmetros\n",
    "- Não precisa de muito pré processamento\n",
    "- Lida bem com dados faltantes, não é necessário imputação\n",
    "    \n",
    "c) **E suas fraquezas?**\n",
    "- Facilmente pode ter overfitting\n",
    "- Alto custo computacional\n",
    "- Devido a alta quantidade de hiperparâmetros, necessita de um GridSearchCV muito grande\n",
    "- Muito difícil a interpretação\n",
    "\n",
    "d) **Como é seu método de avaliação?**\n",
    "      Mesmo método para avaliação de regressão ou classificação\n",
    "   \n",
    "--\n",
    "https://medium.com/equals-lab/uma-breve-introdu%C3%A7%C3%A3o-ao-algoritmo-de-machine-learning-gradient-boosting-utilizando-a-biblioteca-311285783099\n",
    "! https://towardsdatascience.com/all-you-need-to-know-about-gradient-boosting-algorithm-part-1-regression-2520a34a502\n",
    "https://blog.paperspace.com/gradient-boosting-for-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d35af",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9331eeb4",
   "metadata": {},
   "source": [
    "5 - Pensando no algoritimo de Machine Learning \"XGBoosting\" responda:\n",
    "\n",
    "a) **Resumidamente como ele funciona?**\n",
    "É um gradiente boosting, mas com modificações. Em particular, tem-se: (i) a adição de procedimentos de regularização (L1 e L2), o que melhora sua capacidade de generalização e (ii) Utiliza derivadas de segunda ordem para o procedimento de gradiente. Além disso, tem uma performance mais eficiente que o gradient boosting.\n",
    "Pode-se observar que muitas modificações feitas em XGBoost contorna as fraquezas de Gradient Boosting.\n",
    "\n",
    "b) **Quais são suas forças, o que o difere dos outros algoritmos?**\n",
    "Além dos comentados anteriormente, como regularização e utlização de derivadas de segunda ordem, não necessita de imputer, tem otimização de hardware e possui validação cruzada integrada.\n",
    "Pode ser utilizada tanto para Regressão quanto para Classificação \n",
    "\n",
    "c) **E suas fraquezas?**\n",
    "??\n",
    "\n",
    "d) **Como é seu método de avaliação?**\n",
    "Mesma avaliação de Regressão e Classificação\n",
    "\n",
    "--\n",
    "https://www.kdnuggets.com/2019/05/xgboost-algorithm.html\n",
    "! https://medium.com/analytics-vidhya/what-makes-xgboost-so-extreme-e1544a4433bb\n",
    "! https://xgboost.readthedocs.io/en/latest/tutorials/model.html\n",
    "! https://zhanpengfang.github.io/418home.html\n",
    "! https://towardsdatascience.com/xgboost-fine-tune-and-optimize-your-model-23d996fab663\n",
    "! https://iaexpert.academy/2019/04/18/xgboost-a-evolucao-das-arvores-de-decisao/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537f8d7a",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1974de9",
   "metadata": {},
   "source": [
    "6 - Pensando no algoritimo de Machine Learning \"K-Means\" responda:\n",
    "\n",
    "a) **Resumidamente como ele funciona?**\n",
    "É uma técnica de Machine Learning não supervisionada na qual os conjunto de dados são subdivididos em um conjunto de k-grupos (onde k é definido pelo analista). O KMeans tenta separar as amostras em k grupos de mesma variância, minimizando o critério de *inércia* ou *within-cluster-sum-of-squares (WCSS)*. O algoritmo requer que o número de clusters seja especificado.\n",
    "O KMeans escolhe centróides que minimizam a inércia:\n",
    "$$\\sum\\limits_{i=0}^{n}\\min_{\\mu_j\\in C}(\\|x_i-\\mu_j\\|^2)$$\n",
    "Onde:\n",
    "    - C é um cluster especifico;\n",
    "    - $\\mu_j$ é o centróide do cluster\n",
    "    - n são o número de elementos dentro do cluster\n",
    "Inércia pode ser vista como uma medida de como o cluster é internamente coerente.\n",
    "no sklearn: esimator.inertia_\n",
    "\n",
    "b) **Quais são suas forças, o que o difere dos outros algoritmos?**\n",
    "Escala bem para um grande número de amostras\n",
    "Tem aplicações em várias áreas\n",
    "\n",
    "c) **E suas fraquezas?**\n",
    "- O próprio princípio de inércia causa várias desvantagens. A inércia parte da suposição que os clusters são convexos e isotrópicos, o que nem sempre é o caso. Não responde bem para clusters alongados ou com formatos irregulares.\n",
    "- Inércia não é uam métrica normalizada, nós somente sabemos que valores baixos são melhores e zero é o valor ótimo. Mas em espaços de dimensões mais altas, distâncias euclidianas tendem a se tornar inflacionadas (valores extremamente altas ou maldição da dimensionalidade)\n",
    "- Os dados são sensíveis a outliers\n",
    "- k deve ser conhecido anteriormente (conhecimento do negócio)\n",
    "\n",
    "d) **Como é seu método de avaliação?**\n",
    "Apesar da inércia ser menor , mais eficiente é a clusterização, existe um limite de k a ser definido (caso tenda a zero, cada k tem um único elemento e não temos a clusterização). Assim, buscamos um k que desacelere a queda da inércia.\n",
    "Plotamos a inérica (WCSS) em função de k e apontamos o k em que o gráfico deixa de ser tão íngreme. Esse ponto são visualizados como uma quina ou **cotovelo**. Pode-se usar plotar utilizando um loop ou a classe **yellow.cluster.elbow.kelbow_visualizer**.\n",
    "Outro método, mais preciso é o da **Silhoutte (s)**. Nesse caso, o Silhoutte_score fornece um valor entre -1 e 1, onde mais próximo de 1, o ponto está no cluster certo, mais próximo de -1 está no cluster errado e cluster sobrepostos tem valor 0.\n",
    "Com o **yellowbrick.cluster.SilhouetteVisualizer** as silhuetas devem estar acima da média geral, não devem estar no campo negativo e a espessura devem ser similares, no entanto, somente com o silhouette_score pode ser mais preciso. \n",
    "\n",
    "--\n",
    "! https://scikit-learn.org/stable/modules/clustering.html#k-means\n",
    "! https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d845620",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089e0876",
   "metadata": {},
   "source": [
    "7 - Pensando no algoritimo de Machine Learning \"DBScan\" responda:\n",
    "\n",
    "a) **Resumidamente como ele funciona?**\n",
    "DBSCAN é um algoritmo de clusterização baseado no conceito de **densidade**. O algoritmo foi proposto para criar clusters mesmo quando os clusters não forem uniformes, tendo tamanho, forma e densidade variáveis. O princípio fundamental é a **determinação de regiões de alta densidade de observações, que são separadas entre si por regiões de baixa densidade**.\n",
    "A densidade em um ponto seria o número de pontos dentro de um círculo de raio $\\epsilon$ e centrado no ponto P. E uma região densa é se o círculo de raio $\\epsilon$ contém, pelo menos, **minPts** pontos.\n",
    "O algoritmo funciona assim:\n",
    "\n",
    "Passo 1: o algoritmo escolhe aleatoriamente um dos pontos, e sua vizinhança-$\\epsilon$ é calculada;\n",
    "\n",
    "Passo 2: se este ponto tem **minPts** em sua vizinhança-$\\epsilon$, a formação do cluster é iniciada (veja próximo passo). Se não, o ponto é marcado como outlier (mas pode ser considerado como border point de um outro cluster posteriormente). Se for um outlier, volte ao passo 1;\n",
    "\n",
    "Passo 3: se o ponto for um core point, todos os pontos na vizinhança são agregados ao cluster, e o passo 1 é aplicado a cada um deles;\n",
    "\n",
    "Passo 4: o processo do passo 3 é continuado até que todos os pontos tenham um cluster associado, ou seja marcado como noise.\n",
    "\n",
    "b) **Quais são suas forças, o que o difere dos outros algoritmos?**\n",
    "Funciona bem para detecção de ruídos/outliers sem influenciar a detecção de clusters\n",
    "Não necessita determinar a quantidade de clusters (k)\n",
    "Acha clusters de tamanhos e formatos arbitrários\n",
    "\n",
    "c) **E suas fraquezas?**\n",
    "Não funciona bem quanto os outros quando os clusters tem **densidade variável**, isso porque $\\epsilon$ e **minPts** são constantes.\n",
    "Dados com dimensões muito elevadas são mais difíceis de estimar.\n",
    "\n",
    "d) **Como é seu método de avaliação?**\n",
    "???\n",
    "\n",
    "--\n",
    "Anotações de aula - aula05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f989e36d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a5fd5b",
   "metadata": {},
   "source": [
    "8 - Pensando no algoritimo de Machine Learning \"Agglomerative Clustering\" responda:\n",
    "\n",
    "a) **Resumidamente como ele funciona?**\n",
    "Está inserido nos algoritmos chamados \"Hierarchical Clustering\". É um método de analisar clusters de baixo para cima. Esse algoritmo trata cada ponto de dados como um cluster com um elemento e mesclam os clusters mais próximos, subindo na hierarquia até que todos os clusters tenham sido mesclados e haja apenas um único cluster com todos os dados do dataset.\n",
    "Algoritmo:\n",
    "\n",
    "Passo 1. Trate cada ponto de dados como um único cluster.\n",
    "Passo 2. Escolha uma medida de similaridade (linkage geralmente é o 'ward' qeu minimiza a variância)\n",
    "Passo 3. Combine os dois clusters com a menor similaridade (de acordo com a medida escolhida)\n",
    "Passo 4. Repita Passo 3 até que se tenha um cluster\n",
    "Passo 5. Escolha quanto aglomerados queremos olhando para o dendograma (n_clusters)\n",
    "\n",
    "b) **Quais são suas forças, o que o difere dos outros algoritmos?**\n",
    "Não exige que se especifique o número de clusters\n",
    "Todas as métricas de distância funcionam muito bem\n",
    "Fornece uma estrutura hierárquica\n",
    "\n",
    "c) **E suas fraquezas?**\n",
    "Têm menor eficiência, com complexidade de tempo de O(n³)\n",
    "\n",
    "d) **Como é seu método de avaliação?**\n",
    "Podemos usar o Silhouette_score ou mesmo o dendrograma\n",
    "**scipy.cluster.hierarchy.dendrogram**\n",
    "**scipy.cluster.hierarchy.linkage**\n",
    "Depois setar o valor de n_clusters\n",
    "**sklearn.cluster.AgglomerativeClustering(n_clusters=4, linkage='ward')**\n",
    "\n",
    "--\n",
    "Anotações de aula - aula06"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca130de",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d627ee5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
