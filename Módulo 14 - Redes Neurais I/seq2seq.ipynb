{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf7ec981",
   "metadata": {},
   "source": [
    "### Modelos Sequenciais\n",
    "\n",
    "Os Modelos Sequenciais (mais conhecidos como Sequence-to-sequence ou pela sigla seq2seq) é uma metodologia baseada em redes neurais onde utiliza-se deste tipo de modelo para obter uma sequência de entrada em um determinado domínio e converta ela para uma representação em outro domínio específico. A partir das sequências de saída basicamente o modelo monta uma distribuição de probabilidade do que possa ser a sequência de saída através da sequência anterior como entrada.\n",
    "\n",
    "Exemplos clássicos de modelos seq2seq são os tradutores de idiomas, autocomplete (quando ao digitar em um buscador, é sugerir o que a pessoa está querendo escrever) e modelos de autocorrect (dado que a pessoa possa ter digitado algo errado, o modelo sugerir a grafia correta).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cc312c",
   "metadata": {},
   "source": [
    "### Entendendo melhor\n",
    "\n",
    "* Duas redes (encoder e decoder)\n",
    "* Cada rede é responsável por uma tarefa\n",
    "* O aprendizado do decoder depende do que o encoder aprendeu\n",
    "\n",
    "![Title](imgs/seq2seq.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98277440",
   "metadata": {},
   "source": [
    "### Modelos de atenção\n",
    "\n",
    "* Embora eficientes, os modelos seq2seq baseados apenas em encoder/decoder possuem uma camada intermediária S baseada em um único vetor\n",
    "* Muitas informações linguísticas armazenadas apenas neste vetor\n",
    "* Algoritmo tende a degradar quando tem longas sequências\n",
    "* Os modelos seq2seq baseados em atenção tratam a saída do encoder para filtrar as informações mais importantes a serem aprendidas\n",
    "* Possui uma rede neural intermediária para aprender estes pontos de foco\n",
    "![Title](imgs/attention.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e887005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas Auxiliares\n",
    "import string\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "# Bibliotecas de Deep Learning\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed0f959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>portuguese</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vai.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vá.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Oi.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Corre!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Corra!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english portuguese                                               info\n",
       "0     Go.       Vai.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "1     Go.        Vá.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "2     Hi.        Oi.  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
       "3    Run!     Corre!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
       "4    Run!     Corra!  CC-BY 2.0 (France) Attribution: tatoeba.org #9..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/seq2seq.txt', sep='\\t', header=None,\n",
    "                   names=['english', 'portuguese', 'info'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93b5c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[1000: 20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e80b3020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para limpar os textos\n",
    "def clean_sentence(sentence):\n",
    "    # Deixa todas as palavras minúsculas\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # Remove acentuações\n",
    "    sentence = unidecode(sentence)\n",
    "\n",
    "    # Remove pontuações\n",
    "    clean_sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Retorna as sentenças limpas\n",
    "    return clean_sentence\n",
    "\n",
    "# função para tokenização\n",
    "def tokenize(sentences):\n",
    "    # Instância o Tokenizer\n",
    "    text_tokenizer = Tokenizer()\n",
    "\n",
    "    # Treino com os textos\n",
    "    text_tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "    # Retorna os textos tokenizados\n",
    "    return text_tokenizer.texts_to_sequences(sentences), text_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5737a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"port_clean\"] = df['portuguese'].apply(clean_sentence)\n",
    "df[\"eng_clean\"] = df['english'].apply(clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2fa3cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>portuguese</th>\n",
       "      <th>info</th>\n",
       "      <th>port_clean</th>\n",
       "      <th>eng_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>It's a TV.</td>\n",
       "      <td>É uma TV.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "      <td>e uma tv</td>\n",
       "      <td>its a tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>It's a TV.</td>\n",
       "      <td>Isso é uma TV.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "      <td>isso e uma tv</td>\n",
       "      <td>its a tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>It's a TV.</td>\n",
       "      <td>Isto é uma TV.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "      <td>isto e uma tv</td>\n",
       "      <td>its a tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>It's cool.</td>\n",
       "      <td>É legal.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "      <td>e legal</td>\n",
       "      <td>its cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>It's done!</td>\n",
       "      <td>Pronto, já está!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "      <td>pronto ja esta</td>\n",
       "      <td>its done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>Are you a teacher?</td>\n",
       "      <td>Você é professor?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "      <td>voce e professor</td>\n",
       "      <td>are you a teacher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>Are you a teacher?</td>\n",
       "      <td>Tu és professor?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "      <td>tu es professor</td>\n",
       "      <td>are you a teacher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>Are you all right?</td>\n",
       "      <td>Você está bem?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "      <td>voce esta bem</td>\n",
       "      <td>are you all right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>Are you all right?</td>\n",
       "      <td>Vocês estão bem?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "      <td>voces estao bem</td>\n",
       "      <td>are you all right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>Are you all right?</td>\n",
       "      <td>Você está legal?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "      <td>voce esta legal</td>\n",
       "      <td>are you all right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19001 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  english         portuguese  \\\n",
       "1000           It's a TV.          É uma TV.   \n",
       "1001           It's a TV.     Isso é uma TV.   \n",
       "1002           It's a TV.     Isto é uma TV.   \n",
       "1003           It's cool.           É legal.   \n",
       "1004           It's done!   Pronto, já está!   \n",
       "...                   ...                ...   \n",
       "19996  Are you a teacher?  Você é professor?   \n",
       "19997  Are you a teacher?   Tu és professor?   \n",
       "19998  Are you all right?     Você está bem?   \n",
       "19999  Are you all right?   Vocês estão bem?   \n",
       "20000  Are you all right?   Você está legal?   \n",
       "\n",
       "                                                    info        port_clean  \\\n",
       "1000   CC-BY 2.0 (France) Attribution: tatoeba.org #4...          e uma tv   \n",
       "1001   CC-BY 2.0 (France) Attribution: tatoeba.org #4...     isso e uma tv   \n",
       "1002   CC-BY 2.0 (France) Attribution: tatoeba.org #4...     isto e uma tv   \n",
       "1003   CC-BY 2.0 (France) Attribution: tatoeba.org #2...           e legal   \n",
       "1004   CC-BY 2.0 (France) Attribution: tatoeba.org #1...    pronto ja esta   \n",
       "...                                                  ...               ...   \n",
       "19996  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  voce e professor   \n",
       "19997  CC-BY 2.0 (France) Attribution: tatoeba.org #2...   tu es professor   \n",
       "19998  CC-BY 2.0 (France) Attribution: tatoeba.org #2...     voce esta bem   \n",
       "19999  CC-BY 2.0 (France) Attribution: tatoeba.org #2...   voces estao bem   \n",
       "20000  CC-BY 2.0 (France) Attribution: tatoeba.org #2...   voce esta legal   \n",
       "\n",
       "               eng_clean  \n",
       "1000            its a tv  \n",
       "1001            its a tv  \n",
       "1002            its a tv  \n",
       "1003            its cool  \n",
       "1004            its done  \n",
       "...                  ...  \n",
       "19996  are you a teacher  \n",
       "19997  are you a teacher  \n",
       "19998  are you all right  \n",
       "19999  are you all right  \n",
       "20000  are you all right  \n",
       "\n",
       "[19001 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "499453d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words\n",
    "pt_text_tokenized, pt_text_tokenizer = tokenize(df['port_clean'])\n",
    "eng_text_tokenized, eng_text_tokenizer = tokenize(df['eng_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41a34ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maior sentença Português: 8\n",
      "Maior sentença Inglês: 5\n",
      "Vocabulário em Português têm 5655 palavras distintas\n",
      "Vocabulário em Inglês têm 3269 palavras distintas\n"
     ]
    }
   ],
   "source": [
    "# Print das palavras Distintas\n",
    "print('Maior sentença Português: {}'.format(len(max(pt_text_tokenized, key = len))))\n",
    "print('Maior sentença Inglês: {}'.format(len(max(eng_text_tokenized, key = len))))\n",
    "\n",
    "# Checa o tamanho dos vocabulários\n",
    "port_vocab = len(pt_text_tokenizer.word_index) + 1\n",
    "english_vocab = len(eng_text_tokenizer.word_index) + 1\n",
    "\n",
    "# Print do tamanho dos Vocabulários\n",
    "print(\"Vocabulário em Português têm {} palavras distintas\".format(port_vocab))\n",
    "print(\"Vocabulário em Inglês têm {} palavras distintas\".format(english_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6981818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# salva os maiores valores\n",
    "max_port_len = int(len(max(pt_text_tokenized, key = len)))\n",
    "max_english_len = int(len(max(eng_text_tokenized, key = len)))\n",
    "\n",
    "# Cria o pad sequences para as línguas\n",
    "pt_pad_sentence = pad_sequences(pt_text_tokenized, max_port_len, padding = \"post\")\n",
    "eng_pad_sentence = pad_sequences(eng_text_tokenized, max_english_len, padding = \"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbf88db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_port_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc90771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajusta o shape dos dados\n",
    "pt_pad_sentence = pt_pad_sentence.reshape(*pt_pad_sentence.shape, 1)\n",
    "eng_pad_sentence = eng_pad_sentence.reshape(*eng_pad_sentence.shape, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7fa13f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 8, 128)            723840    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 5, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 5, 64)             33024     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 5, 3269)           212485    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 5, 3269)           0         \n",
      "=================================================================\n",
      "Total params: 1,018,757\n",
      "Trainable params: 1,018,757\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Montagem das Camadas da Rede\n",
    "# Camada de Entrada\n",
    "input_sequence = Input(shape = (max_port_len,))\n",
    "# Camada de Embedding\n",
    "embedding = Embedding(input_dim = port_vocab,\n",
    "                      output_dim = 128,)(input_sequence)\n",
    "# Camada Encoder\n",
    "encoder = LSTM(64,\n",
    "               return_sequences = False)(embedding)\n",
    "# Hidden State\n",
    "r_vec = RepeatVector(max_english_len)(encoder)\n",
    "# Decoder\n",
    "decoder = LSTM(64,\n",
    "               return_sequences = True,\n",
    "               dropout = 0.2)(r_vec)\n",
    "# Camada de Saída\n",
    "logits = TimeDistributed(Dense(english_vocab))(decoder)\n",
    "\n",
    "# Define a rede como um modelo\n",
    "enc_dec_model = Model(input_sequence,\n",
    "                      Activation('softmax')(logits))\n",
    "\n",
    "# Compila a Rede\n",
    "enc_dec_model.compile(loss = sparse_categorical_crossentropy,\n",
    "                      optimizer = 'adam',\n",
    "                      metrics = ['accuracy'])\n",
    "\n",
    "# Sumário das Camadas\n",
    "enc_dec_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5da69286",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 17:22:04.090380: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "634/634 [==============================] - 7s 9ms/step - loss: 4.3092 - accuracy: 0.3838\n",
      "Epoch 2/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 3.6928 - accuracy: 0.4182\n",
      "Epoch 3/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 3.4654 - accuracy: 0.4305\n",
      "Epoch 4/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 3.2201 - accuracy: 0.4628\n",
      "Epoch 5/50\n",
      "634/634 [==============================] - 6s 10ms/step - loss: 2.9617 - accuracy: 0.5049\n",
      "Epoch 6/50\n",
      "634/634 [==============================] - 6s 10ms/step - loss: 2.7432 - accuracy: 0.5271\n",
      "Epoch 7/50\n",
      "634/634 [==============================] - 6s 10ms/step - loss: 2.5722 - accuracy: 0.5464\n",
      "Epoch 8/50\n",
      "634/634 [==============================] - 6s 10ms/step - loss: 2.4297 - accuracy: 0.5627\n",
      "Epoch 9/50\n",
      "634/634 [==============================] - 6s 10ms/step - loss: 2.3088 - accuracy: 0.5763\n",
      "Epoch 10/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 2.1948 - accuracy: 0.5901\n",
      "Epoch 11/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 2.0914 - accuracy: 0.6029\n",
      "Epoch 12/50\n",
      "634/634 [==============================] - 6s 10ms/step - loss: 1.9926 - accuracy: 0.6145\n",
      "Epoch 13/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 1.8977 - accuracy: 0.6268\n",
      "Epoch 14/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 1.8103 - accuracy: 0.6360\n",
      "Epoch 15/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 1.7240 - accuracy: 0.6475\n",
      "Epoch 16/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 1.6479 - accuracy: 0.6565\n",
      "Epoch 17/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 1.5753 - accuracy: 0.6663\n",
      "Epoch 18/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 1.5059 - accuracy: 0.6768\n",
      "Epoch 19/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 1.4382 - accuracy: 0.6866\n",
      "Epoch 20/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 1.3749 - accuracy: 0.6957\n",
      "Epoch 21/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 1.3159 - accuracy: 0.7048\n",
      "Epoch 22/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 1.2568 - accuracy: 0.7147\n",
      "Epoch 23/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 1.2019 - accuracy: 0.7263\n",
      "Epoch 24/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 1.1521 - accuracy: 0.7346\n",
      "Epoch 25/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 1.1027 - accuracy: 0.7440\n",
      "Epoch 26/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 1.0535 - accuracy: 0.7544\n",
      "Epoch 27/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 1.0094 - accuracy: 0.7613\n",
      "Epoch 28/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.9729 - accuracy: 0.7685\n",
      "Epoch 29/50\n",
      "634/634 [==============================] - 7s 10ms/step - loss: 0.9317 - accuracy: 0.7770\n",
      "Epoch 30/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.8931 - accuracy: 0.7862\n",
      "Epoch 31/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.8561 - accuracy: 0.7925\n",
      "Epoch 32/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.8276 - accuracy: 0.7989\n",
      "Epoch 33/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.7932 - accuracy: 0.8057\n",
      "Epoch 34/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.7657 - accuracy: 0.8116\n",
      "Epoch 35/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.7355 - accuracy: 0.8187\n",
      "Epoch 36/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.7140 - accuracy: 0.8227\n",
      "Epoch 37/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.6877 - accuracy: 0.8283\n",
      "Epoch 38/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.6643 - accuracy: 0.8336\n",
      "Epoch 39/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.6422 - accuracy: 0.8376\n",
      "Epoch 40/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.6244 - accuracy: 0.8419\n",
      "Epoch 41/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.6033 - accuracy: 0.8463\n",
      "Epoch 42/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.5857 - accuracy: 0.8510\n",
      "Epoch 43/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.5665 - accuracy: 0.8546\n",
      "Epoch 44/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.5534 - accuracy: 0.8572\n",
      "Epoch 45/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.5345 - accuracy: 0.8616\n",
      "Epoch 46/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.5224 - accuracy: 0.8629\n",
      "Epoch 47/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.5073 - accuracy: 0.8664\n",
      "Epoch 48/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.4921 - accuracy: 0.8702\n",
      "Epoch 49/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.4817 - accuracy: 0.8717\n",
      "Epoch 50/50\n",
      "634/634 [==============================] - 6s 9ms/step - loss: 0.4696 - accuracy: 0.8751\n"
     ]
    }
   ],
   "source": [
    "# Fit da Rede\n",
    "model_results = enc_dec_model.fit(pt_pad_sentence,\n",
    "                                  eng_pad_sentence,\n",
    "                                  batch_size = 30, # Tamanho dos pacotes para treino\n",
    "                                  epochs = 50) # número de iterações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8418f796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sentença em Inglês é         : Keep cool.\n",
      "A sentença em Português é      : Te acalma.\n",
      "A sentença predita pelo modelo :\n",
      "keep cool <empty> <empty> <empty>\n"
     ]
    }
   ],
   "source": [
    "# Função para converter os resultados nas sentences\n",
    "def logits_to_sentence(logits, tokenizer):\n",
    "    # Identifica as palavras dentro do tokenizer\n",
    "    index_to_words = {idx: word for word, idx in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<empty>'\n",
    "\n",
    "    # Retorna as predições\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "# Índice para o teste\n",
    "index = 42\n",
    "\n",
    "# Resultados do Modelo\n",
    "print(\"A sentença em Inglês é         : {}\".format(df['english'].iloc[index]))\n",
    "print(\"A sentença em Português é      : {}\".format(df['portuguese'].iloc[index]))\n",
    "print('A sentença predita pelo modelo :')\n",
    "print(logits_to_sentence(enc_dec_model.predict(pt_pad_sentence[index:index+1])[0], eng_text_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e3bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
